  <!-- README.md – paste at repo root -->

<p align="center">
  <img src="https://img.shields.io/badge/Keras-NLP-demo-blue?logo=tensorflow"/>
  <img src="https://img.shields.io/badge/License-MIT-green"/>
  <img src="https://img.shields.io/badge/Colab-ready-✔️-orange"/>
</p>

<h1 align="center">Transformers & NLP</h1>
<h4 align="center">Pre-Trained Inference · Fine-Tuning · Transformer From Scratch (IMDB Sentiment)</h4>


| Task | Goal
|----------|------|------------------------------------|
| **01 – Inference\\_Pretrained** | One-line inference with a KerasNLP preset (`bert_tiny_en_uncased_sst2`)
| **02 – Fine\\_Tune\\_Backbone** | Fine-tune DistilBERT (or BERT-base) on an IMDB subset
| **03 – Transformer\\_From\\_Scratch** | Build & train a 2-layer mini-Transformer (~150 k params)
