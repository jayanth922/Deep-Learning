{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# This model predicts a binary sentiment (1=positive, 0=negative) from short sentences.\n",
        "# It uses a basic Tokenizer/Embedding/GlobalAveragePooling architecture.\n",
        "\n",
        "# Sample training data\n",
        "easy_sentences = [\n",
        "    \"I love this product!\",\n",
        "    \"This is the worst thing I've ever bought.\",\n",
        "    \"Not bad, could be better.\",\n",
        "    \"Absolutely fantastic service.\"\n",
        "]\n",
        "easy_labels = [1, 0, 1, 1]\n",
        "\n",
        "# Tokenize the text.\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 1000\n",
        "oov_token = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(easy_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(easy_sentences)\n",
        "max_length = 10\n",
        "padded_sequences = pad_sequences(sequences, padding=\"post\", maxlen=max_length)\n",
        "\n",
        "# Define a simple sequential model.\n",
        "model_simple = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model_simple.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_simple.build((None, max_length))\n",
        "model_simple.summary()\n",
        "\n",
        "# Train the simple model.\n",
        "history_simple = model_simple.fit(padded_sequences, np.array(easy_labels), epochs=20, batch_size=2)\n",
        "# Predict on new sentences.\n",
        "test_sentences = [\"I really enjoyed this!\", \"Terrible experience.\"]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=\"post\", maxlen=max_length)\n",
        "preds_simple = model_simple.predict(test_padded)\n",
        "print(\"\\nEasy Level Model Predictions:\")\n",
        "print(\"Prediction shape:\", preds_simple.shape, \"DType:\", preds_simple.dtype)\n",
        "print(\"Predicted sentiment probabilities (values closer to 1 indicate positive sentiment):\")\n",
        "print(preds_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxpDWOBN64ef",
        "outputId": "03e06475-b9c5-484f-991b-a2e5116e61ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Easy Level: Sentiment Analysis with Simple Embedding Model ===\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 10, 16)            16000     \n",
            "                                                                 \n",
            " global_average_pooling1d_3   (None, 16)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,289\n",
            "Trainable params: 16,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "2/2 [==============================] - 2s 8ms/step - loss: 0.6844 - accuracy: 0.7500\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6799 - accuracy: 0.7500\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6736 - accuracy: 0.7500\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6691 - accuracy: 0.7500\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6623 - accuracy: 0.7500\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6576 - accuracy: 0.7500\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6506 - accuracy: 0.7500\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6446 - accuracy: 0.7500\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6396 - accuracy: 0.7500\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6334 - accuracy: 0.7500\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6260 - accuracy: 0.7500\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6197 - accuracy: 0.7500\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6143 - accuracy: 0.7500\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6076 - accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6007 - accuracy: 0.7500\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5928 - accuracy: 0.7500\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5867 - accuracy: 0.7500\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5783 - accuracy: 0.7500\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5718 - accuracy: 0.7500\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5632 - accuracy: 0.7500\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "\n",
            "Easy Level Model Predictions:\n",
            "Prediction shape: (2, 1) DType: float32\n",
            "Predicted sentiment probabilities (values closer to 1 indicate positive sentiment):\n",
            "[[0.5824754]\n",
            " [0.595556 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This model fine-tunes BERT to predict sentiment from raw text.\n",
        "# We force the BERT preprocessor to run on CPU to avoid resource conflicts.\n",
        "\n",
        "# Sample training data.\n",
        "intermediate_sentences = [\n",
        "    \"I loved the movie. It was fantastic!\",\n",
        "    \"The film was boring and too long.\",\n",
        "    \"What an excellent performance!\",\n",
        "    \"Terrible movie. I hated it.\"\n",
        "]\n",
        "intermediate_labels = [1, 0, 1, 0]\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text  # Make sure this version is compatible with your TF version\n",
        "\n",
        "# Custom wrapper for BERT preprocessor.\n",
        "class WrappedBERTPreprocessor(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(WrappedBERTPreprocessor, self).__init__(**kwargs)\n",
        "        self.preprocessor_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
        "            name=\"bert_preprocess\"\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        # Force the preprocessor to run on CPU.\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            return self.preprocessor_layer(inputs)\n",
        "\n",
        "# Define the BERT sentiment model.\n",
        "input_text_bert = tf.keras.Input(shape=(), dtype=tf.string, name=\"input_text\")\n",
        "wrapped_preprocessor = WrappedBERTPreprocessor()(input_text_bert)\n",
        "encoder = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n",
        "    trainable=True, name=\"bert_encoder\"\n",
        ")\n",
        "# Get the pooled output from the BERT encoder.\n",
        "encoder_outputs = encoder(wrapped_preprocessor)\n",
        "x_bert = encoder_outputs['pooled_output']\n",
        "x_bert = tf.keras.layers.Dropout(0.1)(x_bert)\n",
        "output_bert = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classifier\")(x_bert)\n",
        "model_bert = tf.keras.Model(inputs=input_text_bert, outputs=output_bert, name=\"BERT_Sentiment_Model\")\n",
        "model_bert.build((None,))\n",
        "model_bert.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "                   loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_bert.summary()\n",
        "\n",
        "# Train the BERT model.\n",
        "model_bert.fit(tf.constant(intermediate_sentences), np.array(intermediate_labels), epochs=2, batch_size=2)\n",
        "# Predict with the BERT model.\n",
        "preds_bert = model_bert.predict(tf.constant([\n",
        "    \"An amazing experience, I would watch it again.\",\n",
        "    \"It was a waste of time.\"\n",
        "]))\n",
        "print(\"\\nIntermediate Level Model Predictions:\")\n",
        "print(\"Prediction shape:\", preds_bert.shape, \"DType:\", preds_bert.dtype)\n",
        "print(\"Predicted sentiment probabilities (closer to 1 indicates positive sentiment):\")\n",
        "print(preds_bert)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acIG-hxZ7b6s",
        "outputId": "34072797-3c49-4e95-af6e-1381ea7715f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Intermediate Level: Fine-Tuning BERT for Sentiment Classification ===\n",
            "Model: \"BERT_Sentiment_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_text (InputLayer)        [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " wrapped_bert_preprocessor_2 (W  {'input_type_ids':   0          ['input_text[0][0]']             \n",
            " rappedBERTPreprocessor)        (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128),                                                          \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " bert_encoder (KerasLayer)      {'pooled_output': (  109482241   ['wrapped_bert_preprocessor_2[0][\n",
            "                                None, 768),                      0]',                             \n",
            "                                 'sequence_output':               'wrapped_bert_preprocessor_2[0][\n",
            "                                 (None, 128, 768),               1]',                             \n",
            "                                 'encoder_outputs':               'wrapped_bert_preprocessor_2[0][\n",
            "                                 [(None, 128, 768),              2]']                             \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'default': (None,                                                \n",
            "                                768)}                                                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 768)          0           ['bert_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " classifier (Dense)             (None, 1)            769         ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 109,483,009\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "2/2 [==============================] - 51s 6s/step - loss: 0.8951 - accuracy: 0.5000\n",
            "Epoch 2/2\n",
            "2/2 [==============================] - 8s 4s/step - loss: 0.5380 - accuracy: 0.7500\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "\n",
            "Intermediate Level Model Predictions:\n",
            "Prediction shape: (2, 1) DType: float32\n",
            "Predicted sentiment probabilities (closer to 1 indicates positive sentiment):\n",
            "[[0.32621172]\n",
            " [0.20140675]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required resources. We add 'punkt_tab' to resolve the error.\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "# Define a sample text.\n",
        "text_for_ner = (\n",
        "    \"Apple Inc. is looking at buying a startup in the U.K. for $1 billion. \"\n",
        "    \"Tim Cook, the CEO of Apple, stated that this acquisition will strengthen the company's market position.\"\n",
        ")\n",
        "\n",
        "# Tokenize the text into sentences.\n",
        "sentences = nltk.sent_tokenize(text_for_ner)\n",
        "\n",
        "# For each sentence, tokenize into words, perform POS tagging, and then perform NER.\n",
        "print(\"\\nDetected Named Entities:\")\n",
        "for sentence in sentences:\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    tree = nltk.ne_chunk(pos_tags)\n",
        "    # Traverse the tree and print named entities.\n",
        "    for subtree in tree:\n",
        "        if hasattr(subtree, 'label'):\n",
        "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            entity_type = subtree.label()\n",
        "            print(f\"{entity}: {entity_type}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-6chMoz7dbe",
        "outputId": "502fe29a-3a30-4d01-973d-02b262d3f1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Advanced Level: Named Entity Recognition (NER) using NLTK ===\n",
            "\n",
            "Detected Named Entities:\n",
            "Apple: PERSON\n",
            "Inc.: ORGANIZATION\n",
            "Tim: PERSON\n",
            "Cook: GPE\n",
            "CEO: ORGANIZATION\n",
            "Apple: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install transformers if not already installed.\n",
        "!pip install transformers --quiet\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a summarization pipeline using T5-small.\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "# Provide a long text to summarize.\n",
        "text_to_summarize = (\n",
        "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, \"\n",
        "    \"in contrast to the natural intelligence displayed by humans and animals. \"\n",
        "    \"Leading AI textbooks define the field as the study of 'intelligent agents': any device \"\n",
        "    \"that perceives its environment and takes actions that maximize its chance of successfully \"\n",
        "    \"achieving its goals. Colloquially, the term 'artificial intelligence' is often used to \"\n",
        "    \"describe machines (or computers) that mimic cognitive functions that humans associate with \"\n",
        "    \"the human mind, such as learning and problem-solving.\"\n",
        ")\n",
        "summary = summarizer(text_to_summarize, max_length=60, min_length=20, do_sample=False)\n",
        "print(\"\\nExpert Level Text Summarization Output:\")\n",
        "print(\"Summarized Text:\", summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBKxNA62JXER",
        "outputId": "9dfce5ae-24f4-46b9-ffff-7ced9b3e3542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Expert Level: Text Summarization with T5 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Expert Level Text Summarization Output:\n",
            "Summarized Text: leading AI textbooks define the field as the study of 'intelligent agents' the term 'artificial intelligence' is often used to describe machines that mimic cognitive functions that humans associate with the human mind .\n"
          ]
        }
      ]
    }
  ]
}