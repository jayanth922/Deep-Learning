{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZtVDOzCadF6",
        "outputId": "34991371-b506-4937-95a4-82305c53e3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor x: tensor([1, 2, 3])\n",
            "Tensor y:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Reshaped tensor z (3,2):\n",
            " tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "Transpose of z:\n",
            " tensor([[1, 3, 5],\n",
            "        [2, 4, 6]])\n",
            "Element-wise square of x: tensor([1, 4, 9])\n",
            "Matrix multiplication result:\n",
            " tensor([[ 4,  4],\n",
            "        [10,  8]])\n",
            "Einsum (elementwise product sum) result: 70.0\n",
            "Matrix multiplication using einsum:\n",
            " tensor([[ 4,  4],\n",
            "        [10,  8]])\n",
            "Broadcast addition result:\n",
            " tensor([[11, 12, 13],\n",
            "        [21, 22, 23],\n",
            "        [31, 32, 33]])\n",
            "Concatenated tensor: tensor([1, 2, 3, 4, 5, 6])\n",
            "Stacked tensor:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Original tensor shape: torch.Size([2, 3, 4])\n",
            "Rearranged tensor shape using einops: torch.Size([6, 4])\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # Exploring Tensor Operations in PyTorch\n",
        "#\n",
        "# This notebook covers several basic and some less common tensor operations in PyTorch.\n",
        "# We include:\n",
        "# - Tensor creation, reshaping, and transposition.\n",
        "# - Elementwise and matrix multiplication operations.\n",
        "# - Demonstrations of `torch.einsum` for flexible tensor operations.\n",
        "# - Additional operations such as concatenation, stacking, and use of the einops library.\n",
        "#\n",
        "# For additional reference:\n",
        "# - [PyTorch Tensors - Deep Learning with PyTorch](https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch3/1_tensors.ipynb) :contentReference[oaicite:2]{index=2}\n",
        "# - [PyTorch Tensor Operations Presentation](https://docs.google.com/presentation/d/13Oo5gXwcsoq9oMC4XriAyxkvgicatBxfI4cZzDhRyiE/edit#slide=id.p) :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "# %% [code]\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create basic tensors\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "print(\"Tensor x:\", x)\n",
        "print(\"Tensor y:\\n\", y)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Reshaping and Transposing\n",
        "#\n",
        "# PyTorch makes it easy to reshape and transpose tensors.\n",
        "#\n",
        "\n",
        "# %% [code]\n",
        "# Reshape tensor y from (2,3) to (3,2)\n",
        "z = y.view(3, 2)\n",
        "print(\"Reshaped tensor z (3,2):\\n\", z)\n",
        "\n",
        "# Transpose of z (for 2D, .t() is available)\n",
        "print(\"Transpose of z:\\n\", z.t())\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Elementwise Operations and Matrix Multiplication\n",
        "#\n",
        "\n",
        "# %% [code]\n",
        "# Elementwise square of tensor x\n",
        "square_x = x * x\n",
        "print(\"Element-wise square of x:\", square_x)\n",
        "\n",
        "# Matrix multiplication example\n",
        "m1 = torch.tensor([[1, 2], [3, 4]])\n",
        "m2 = torch.tensor([[2, 0], [1, 2]])\n",
        "matmul_result = torch.mm(m1, m2)\n",
        "print(\"Matrix multiplication result:\\n\", matmul_result)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Demonstrating Einstein Summation (einsum)\n",
        "#\n",
        "# Using `torch.einsum` we perform:\n",
        "#\n",
        "# 1. **Sum of elementwise product:** Computes the sum of products over all corresponding elements.\n",
        "# 2. **Matrix multiplication using einsum.**\n",
        "#\n",
        "\n",
        "# %% [code]\n",
        "# Example 1: Sum over elementwise product of two matrices\n",
        "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
        "einsum_result = torch.einsum('ij,ij->', A, B)\n",
        "print(\"Einsum (elementwise product sum) result:\", einsum_result.item())\n",
        "\n",
        "# Example 2: Matrix multiplication using einsum\n",
        "einsum_matmul = torch.einsum('ik,kj->ij', m1, m2)\n",
        "print(\"Matrix multiplication using einsum:\\n\", einsum_matmul)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Additional Tensor Operations\n",
        "#\n",
        "# Here we explore some additional operations:\n",
        "#\n",
        "# - **Broadcasting addition:** Automatically expands dimensions.\n",
        "# - **Concatenation and stacking:** Combine tensors along various dimensions.\n",
        "# - **Using einops:** For intuitive tensor dimension reordering.\n",
        "#\n",
        "\n",
        "# %% [code]\n",
        "# Broadcasting addition example\n",
        "tensor1 = torch.tensor([[1, 2, 3]])\n",
        "tensor2 = torch.tensor([[10], [20], [30]])\n",
        "broadcast_add = tensor1 + tensor2  # This will broadcast to shape (3,3)\n",
        "print(\"Broadcast addition result:\\n\", broadcast_add)\n",
        "\n",
        "# Concatenation and stacking examples\n",
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "tensor_b = torch.tensor([4, 5, 6])\n",
        "concatenated = torch.cat((tensor_a, tensor_b), dim=0)\n",
        "stacked = torch.stack((tensor_a, tensor_b), dim=0)\n",
        "print(\"Concatenated tensor:\", concatenated)\n",
        "print(\"Stacked tensor:\\n\", stacked)\n",
        "\n",
        "# %% [code]\n",
        "# Optional: Using einops for more expressive tensor operations\n",
        "try:\n",
        "    from einops import rearrange\n",
        "    # Create a random tensor with shape (batch, seq, features)\n",
        "    tensor = torch.randn(2, 3, 4)\n",
        "    # Rearrange to combine batch and sequence dimensions\n",
        "    rearranged = rearrange(tensor, 'b n d -> (b n) d')\n",
        "    print(\"Original tensor shape:\", tensor.shape)\n",
        "    print(\"Rearranged tensor shape using einops:\", rearranged.shape)\n",
        "except ImportError:\n",
        "    print(\"einops not installed. You can install it via `pip install einops`.\")\n"
      ]
    }
  ]
}