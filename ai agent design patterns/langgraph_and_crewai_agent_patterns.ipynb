{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf3e0224",
      "metadata": {
        "id": "cf3e0224"
      },
      "source": [
        "# Effective Agent Patterns with **LangGraph** + Groq\n",
        "\n",
        "This Colab demonstrates **all seven artefacts** referenced in Anthropic‚Äôs *Building‚ÄØEffective‚ÄØAgents* framework and the LangGraph tutorial:\n",
        "\n",
        "| Layer | Pattern / Artefact | Section |\n",
        "|-------|--------------------|---------|\n",
        "| Building‚Äëblock | **Augmented‚ÄØLLM (Tool¬†Use)** | 1 |\n",
        "| Workflows | Prompt‚ÄëChaining, Parallelization, Routing, Orchestrator‚ÄëWorker, Evaluator‚ÄëOptimizer | 2‚Äë6 |\n",
        "| Agent archetype | **ReAct Agent** (autonomous, tool‚Äëcalling loop) | 7 |\n",
        "\n",
        "The notebook auto‚Äëlogs every run to **LangSmith**, giving you shareable debug traces for recording.\n",
        "\n",
        "> **Prerequisites**\n",
        "> * A Groq Cloud key in Colab‚Äôs Secrets (`GROQ_API_KEY`).\n",
        "> * *(Optional)* A LangSmith API key (`LANGCHAIN_API_KEY`).  \n",
        "> * GPU runtime is **not** required.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47be351e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47be351e",
        "outputId": "6e545ed6-4473-45ce-ab9c-2e0389a2fa9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: langgraph<0.4,>=0.3.31 in /usr/local/lib/python3.11/dist-packages (0.3.34)\n",
            "Requirement already satisfied: langchain-groq>=0.3.2 in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph<0.4,>=0.3.31) (0.3.55)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph<0.4,>=0.3.31) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from langgraph<0.4,>=0.3.31) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph<0.4,>=0.3.31) (0.1.63)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph<0.4,>=0.3.31) (3.5.0)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq>=0.3.2) (0.23.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq>=0.3.2) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq>=0.3.2) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq>=0.3.2) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq>=0.3.2) (4.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph<0.4,>=0.3.31) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph<0.4,>=0.3.31) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph<0.4,>=0.3.31) (6.0.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph<0.4,>=0.3.31) (1.9.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph<0.4,>=0.3.31) (3.0.0)\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.11/dist-packages (8.0.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.2)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.55)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.31)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "‚úÖ Groq LLM initialised\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title üîß¬†Install & initialise\n",
        "!pip install -U pip        # optional but avoids resolver quirks\n",
        "!pip install \\\n",
        "    \"langgraph>=0.3.31,<0.4\" \\\n",
        "    \"langchain-groq>=0.3.2\" \\\n",
        "    langsmith\n",
        "!pip install -U duckduckgo-search\n",
        "!pip install langchain-community  # Install the missing package\n",
        "\n",
        "from google.colab import userdata\n",
        "import os, sys\n",
        "\n",
        "# Environment variables1\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\") or \"\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ.setdefault(\"LANGCHAIN_PROJECT\", \"agent_patterns_demo\")\n",
        "if userdata.get(\"LANGCHAIN_API_KEY\"):\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0)\n",
        "print(\"‚úÖ Groq LLM initialised\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4992426c",
      "metadata": {
        "id": "4992426c"
      },
      "source": [
        "## 1‚ÄØ¬∑‚ÄØAugmented‚ÄØLLM¬†‚Äî using tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b63ba8a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b63ba8a7",
        "outputId": "13f27f65-9809-4ee3-89ad-2e4274ee9e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 96\n"
          ]
        }
      ],
      "source": [
        "# Define simple calculator tools with docstrings\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Return the product of two integers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Return the sum of two integers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a by b and return a float (raises if b == 0).\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "tools = [multiply, add, divide]\n",
        "llm_tools = llm.bind_tools(tools)\n",
        "tools_map = {t.name: t for t in tools}\n",
        "\n",
        "# 2) LLM node: ALWAYS prepend a SystemMessage\n",
        "def call_llm(state: MessagesState) -> dict:\n",
        "    system = SystemMessage(\n",
        "        content=(\n",
        "            \"You are a calculator agent. \"\n",
        "            \"You must call the provided tools for every single operation in sequence. \"\n",
        "            \"Do NOT compute anything yourself.\"\n",
        "        )\n",
        "    )\n",
        "    # Build the message sequence: system ‚Üí history\n",
        "    msgs = [system] + state[\"messages\"]\n",
        "    out  = llm_tools.invoke(msgs)\n",
        "    return {\"messages\": [out]}\n",
        "\n",
        "# 3) Tool node: same as before\n",
        "def call_tool(state: MessagesState) -> dict:\n",
        "    \"\"\"Execute each pending tool_call using the tool‚Äôs raw Python function.\"\"\"\n",
        "    last      = state[\"messages\"][-1]\n",
        "    results   = []\n",
        "    # tc['name'] is the tool name, tc['args'] is a dict of keyword args\n",
        "    for tc in getattr(last, \"tool_calls\", []):\n",
        "        tool_fn   = tools_map[tc[\"name\"]].func     # get the Python function\n",
        "        output    = tool_fn(**tc[\"args\"])          # call it directly\n",
        "        results.append(\n",
        "            ToolMessage(content=str(output), tool_call_id=tc[\"id\"])\n",
        "        )\n",
        "    return {\"messages\": results}\n",
        "\n",
        "# 4) Dispatcher: unchanged\n",
        "def dispatcher(state: MessagesState) -> str:\n",
        "    last = state[\"messages\"][-1]\n",
        "    if isinstance(last, ToolMessage):\n",
        "        return \"llm\"\n",
        "    if getattr(last, \"tool_calls\", None):\n",
        "        return \"tool\"\n",
        "    return END\n",
        "\n",
        "# 5) Build & invoke\n",
        "g = StateGraph(MessagesState)\n",
        "g.add_node(\"llm\",  call_llm)\n",
        "g.add_node(\"tool\", call_tool)\n",
        "g.add_edge(START,      \"llm\")\n",
        "g.add_conditional_edges(\"llm\", dispatcher, {\"tool\":\"tool\",\"llm\":\"llm\",END:END})\n",
        "g.add_edge(\"tool\",     \"llm\")\n",
        "\n",
        "agent = g.compile()\n",
        "state = agent.invoke({\"messages\":[HumanMessage(content=\"What is 9 * 8 / 6?\")]})\n",
        "state = agent.invoke({\n",
        "    \"messages\": [ HumanMessage(content=\"What is 9 * 8 / 6?\") ]\n",
        "})\n",
        "\n",
        "# Find the last tool output\n",
        "answer = None\n",
        "for msg in reversed(state[\"messages\"]):\n",
        "    if isinstance(msg, ToolMessage):\n",
        "        answer = msg.content\n",
        "        break\n",
        "\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66cb667",
      "metadata": {
        "id": "e66cb667"
      },
      "source": [
        "## 2‚ÄØ¬∑‚ÄØPrompt‚ÄëChaining\n",
        "\n",
        "Break a single task into deterministic sequential sub‚Äëtasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dd32caac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd32caac",
        "outputId": "a061d45f-8104-4620-e337-d07676851fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love it! The added humor is great! The puns are clever and well-executed. I think it's perfect just the way it is.\n",
            "\n",
            "The only thing I might suggest is adding a punchline or a final twist to really drive the humor home. For example:\n",
            "\n",
            "\"Why did the database go to therapy?\n",
            "\n",
            "Because it was feeling a little 'fragmented' and had a lot of 'unresolved queries'! It was struggling to 'connect the dots' and was worried it would 'crash' under the pressure of its own 'data overload'! But in the end, it just needed to 'reboot' its perspective and 'update' its outlook!\"\n",
            "\n",
            "The added punchline about rebooting and updating adds a bit of extra humor and cleverness to the joke. But honestly, the original version is already great, and the added twist is just a suggestion!\n"
          ]
        }
      ],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "    topic: str\n",
        "    joke: str\n",
        "    improved_joke: str\n",
        "    final_joke: str\n",
        "\n",
        "def gen(state): return {\"joke\": llm.invoke(f\"Write a short joke about {state['topic']}\").content}\n",
        "def improve(state): return {\"improved_joke\": llm.invoke(f\"Make funnier: {state['joke']}\").content}\n",
        "def polish(state): return {\"final_joke\": llm.invoke(f\"Add a twist: {state['improved_joke']}\").content}\n",
        "\n",
        "g = StateGraph(State)\n",
        "g.add_node(\"gen\", gen); g.add_node(\"improve\", improve); g.add_node(\"polish\", polish)\n",
        "g.add_edge(START,\"gen\"); g.add_edge(\"gen\",\"improve\"); g.add_edge(\"improve\",\"polish\"); g.add_edge(\"polish\",END)\n",
        "chain = g.compile()\n",
        "print(chain.invoke({\"topic\":\"databases\"})[\"final_joke\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5379bee",
      "metadata": {
        "id": "f5379bee"
      },
      "source": [
        "## 3‚ÄØ¬∑‚ÄØParallelization\n",
        "\n",
        "Run independent sub‚Äëtasks concurrently, then aggregate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b278363",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b278363",
        "outputId": "fae29bd2-e350-4ff3-cc71-a0bf40463681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As the company's data grew exponentially, they realized the need to upgrade their infrastructure to accommodate the increased demand. By migrating to a cloud computing platform, they were able to scale their operations seamlessly, reducing costs and increasing efficiency, allowing them to focus on innovation and growth.\n",
            "---\n",
            "Here is a haiku on cloud computing:\n",
            "\n",
            "Data floats on air\n",
            "Scalable, secure, and free space\n",
            "Innovation's nest\n",
            "---\n",
            "Why did the cloud go to therapy?\n",
            "\n",
            "Because it was feeling a little \"foggy\" and wanted to \"clear the air\" about its \"storage\" issues!\n"
          ]
        }
      ],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class PState(TypedDict):\n",
        "    topic:   str\n",
        "    poem:    str\n",
        "    story:   str\n",
        "    joke:    str\n",
        "    merged:  str\n",
        "\n",
        "def make_poem(s: PState) -> dict:\n",
        "    return {\"poem\": llm.invoke(f\"Haiku on {s['topic']}\").content}\n",
        "\n",
        "def make_story(s: PState) -> dict:\n",
        "    return {\"story\": llm.invoke(f\"Two‚Äësentence story on {s['topic']}\").content}\n",
        "\n",
        "def make_joke(s: PState) -> dict:\n",
        "    return {\"joke\": llm.invoke(f\"Dad joke on {s['topic']}\").content}\n",
        "\n",
        "def aggregate(s: PState) -> dict:\n",
        "    return {\"merged\": f\"{s['story']}\\n---\\n{s['poem']}\\n---\\n{s['joke']}\"}\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "pg = StateGraph(PState)\n",
        "\n",
        "# Use unique node IDs (avoid the exact TypedDict keys)\n",
        "pg.add_node(\"gen_poem\",  make_poem)\n",
        "pg.add_node(\"gen_story\", make_story)\n",
        "pg.add_node(\"gen_joke\",  make_joke)\n",
        "pg.add_node(\"merge_all\", aggregate)\n",
        "\n",
        "# Wire them up\n",
        "pg.add_edge(START,     \"gen_poem\")\n",
        "pg.add_edge(START,     \"gen_story\")\n",
        "pg.add_edge(START,     \"gen_joke\")\n",
        "pg.add_edge(\"gen_poem\",\"merge_all\")\n",
        "pg.add_edge(\"gen_story\",\"merge_all\")\n",
        "pg.add_edge(\"gen_joke\",\"merge_all\")\n",
        "pg.add_edge(\"merge_all\", END)\n",
        "\n",
        "result = pg.compile().invoke({\"topic\":\"cloud computing\"})\n",
        "print(result[\"merged\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ee5394",
      "metadata": {
        "id": "09ee5394"
      },
      "source": [
        "## 4‚ÄØ¬∑‚ÄØRouting\n",
        "\n",
        "Classify input, then dispatch to the correct specialised workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bbfac336",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbfac336",
        "outputId": "e464e8a3-c8c1-413d-c131-198796953ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There once was a AI so fine,\n",
            "Whose learning was truly divine.\n",
            "It processed with ease,\n",
            "All the data it pleased,\n",
            "And made predictions that were sublime.\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing_extensions import Literal, TypedDict\n",
        "class Route(BaseModel): step: Literal[\"poem\",\"story\",\"joke\"]=Field(...)\n",
        "\n",
        "router = llm.with_structured_output(Route)\n",
        "class RState(TypedDict): query:str; answer:str\n",
        "\n",
        "def decide(s):\n",
        "    return router.invoke(f\"Is '{s['query']}' asking for a poem, story or joke?\").step\n",
        "\n",
        "def poem(s): return {\"answer\": llm.invoke(f\"Poem: {s['query']}\").content}\n",
        "def story(s): return {\"answer\": llm.invoke(f\"Story: {s['query']}\").content}\n",
        "def joke(s): return {\"answer\": llm.invoke(f\"Joke: {s['query']}\").content}\n",
        "\n",
        "rg = StateGraph(RState)\n",
        "rg.add_conditional_edges(START, decide, {\"poem\":\"poem\",\"story\":\"story\",\"joke\":\"joke\"})\n",
        "for n,f in [(\"poem\",poem),(\"story\",story),(\"joke\",joke)]: rg.add_node(n,f); rg.add_edge(n,END)\n",
        "print(rg.compile().invoke({\"query\":\"Write a limerick about AI\"})[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f639bdc3",
      "metadata": {
        "id": "f639bdc3"
      },
      "source": [
        "## 5‚ÄØ¬∑‚ÄØOrchestrator‚ÄëWorker\n",
        "\n",
        "Planner decomposes the task; workers execute; synthesiser merges outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "47dca87f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47dca87f",
        "outputId": "d6fc7de1-54ff-4d0e-c050-729b8a58e385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a possible introduction for a white paper on Vector DB Indexing:\n",
            "\n",
            "**Introduction**\n",
            "\n",
            "In today's data-driven world, the ability to efficiently store, retrieve, and query large amounts of data is crucial for businesses and organizations to make informed decisions and stay competitive. With the proliferation of machine learning and artificial intelligence, the volume and complexity of data have increased exponentially, making traditional database indexing techniques inadequate for many appli\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "class Section(BaseModel): title:str; description:str\n",
        "class Plan(BaseModel): sections:List[Section]\n",
        "planner = llm.with_structured_output(Plan)\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from typing import List, Annotated\n",
        "import operator\n",
        "\n",
        "class OWState(TypedDict):\n",
        "    topic:    str\n",
        "    sections: List[Section]\n",
        "    drafts:   Annotated[List[str], operator.add]   # ‚Üê fold multiple lists via list¬†+¬†list\n",
        "    report:   str\n",
        "\n",
        "def orchestrate(s): return {\"sections\": planner.invoke(f\"Plan white‚Äëpaper on {s['topic']}\").sections}\n",
        "\n",
        "def worker(s):\n",
        "    sec=s[\"section\"]\n",
        "    return {\"drafts\":[llm.invoke(f\"Write section '{sec.title}': {sec.description}\").content]}\n",
        "\n",
        "from langgraph.constants import Send\n",
        "def dispatch(s): return [Send(\"write\",{\"section\":sec}) for sec in s[\"sections\"]]\n",
        "\n",
        "def synth(s): return {\"report\":\"\\n\\n\".join(s[\"drafts\"])}\n",
        "\n",
        "g = StateGraph(OWState)\n",
        "g.add_node(\"orchestrate\", orchestrate)\n",
        "g.add_node(\"write\", worker)\n",
        "g.add_node(\"synth\", synth)\n",
        "g.add_edge(START,\"orchestrate\")\n",
        "g.add_conditional_edges(\"orchestrate\", dispatch, [\"write\"])\n",
        "g.add_edge(\"write\",\"synth\"); g.add_edge(\"synth\",END)\n",
        "print(g.compile().invoke({\"topic\":\"Vector¬†DB¬†Indexing\"})[\"report\"][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca3bd5d",
      "metadata": {
        "id": "0ca3bd5d"
      },
      "source": [
        "## 6‚ÄØ¬∑‚ÄØEvaluator‚ÄëOptimizer\n",
        "\n",
        "Generator ‚áÑ Evaluator loop until quality threshold met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f7405230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7405230",
        "outputId": "cf17fa18-8241-42dd-ce00-a7271e95a2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tweet: \"Discover the power of graph-based language models! LangGraph is revolutionizing NLP by leveraging graph neural networks to analyze complex linguistic structures. Unlock new insights into language and improve your AI applications with LangGraph! #LangGraph #NLP #GraphNeuralNetworks\"\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing_extensions import Literal, TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 1) Define your feedback schema\n",
        "class Feedback(BaseModel):\n",
        "    grade:   Literal[\"good\", \"bad\"]\n",
        "    comment: str\n",
        "\n",
        "# 2) Wrap the LLM to output that schema\n",
        "eval_llm = llm.with_structured_output(Feedback)\n",
        "\n",
        "# 3) Define your agent state\n",
        "class EState(TypedDict):\n",
        "    topic:   str\n",
        "    draft:   str\n",
        "    comment: str\n",
        "    grade:   str\n",
        "\n",
        "# 4) Generation node\n",
        "def gen(state: EState) -> dict:\n",
        "    if not state.get(\"comment\"):\n",
        "        prompt = f\"Write a tweet about {state['topic']}\"\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"Rewrite the tweet about {state['topic']} \"\n",
        "            f\"considering: {state['comment']}\"\n",
        "        )\n",
        "    return {\"draft\": llm.invoke(prompt).content}\n",
        "\n",
        "# 5) Evaluation node\n",
        "def judge(state: EState) -> dict:\n",
        "    fb = eval_llm.invoke(\n",
        "        f\"Grade this tweet: '{state['draft']}'. \"\n",
        "        \"Respond with grade and feedback.\"\n",
        "    )\n",
        "    return {\"grade\": fb.grade, \"comment\": fb.comment}\n",
        "\n",
        "# 6) Routing function\n",
        "def route(state: EState) -> str:\n",
        "    # If bad, loop back to 'gen'; otherwise finish.\n",
        "    return \"gen\" if state[\"grade\"] == \"bad\" else END\n",
        "\n",
        "# 7) Wire the graph\n",
        "g = StateGraph(EState)\n",
        "g.add_node(\"gen\",   gen)\n",
        "g.add_node(\"judge\", judge)\n",
        "g.add_edge(START, \"gen\")\n",
        "g.add_edge(\"gen\",   \"judge\")\n",
        "g.add_conditional_edges(\"judge\", route, {\"gen\": \"gen\", END: END})\n",
        "\n",
        "# 8) Compile & invoke\n",
        "tweet_loop = g.compile()\n",
        "result = tweet_loop.invoke({\"topic\": \"LangGraph\"})\n",
        "print(\"Final tweet:\", result[\"draft\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6dcca00",
      "metadata": {
        "id": "c6dcca00"
      },
      "source": [
        "## 7‚ÄØ¬∑‚ÄØReAct Agent (Autonomous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e88e00b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e88e00b0",
        "outputId": "884bc512-daa5-47eb-a52a-32b6ba66f02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 champ answer: Max Verstappen has become Formula 1 world champion for the fourth time after clinching the 2024 title with a fifth-placed finish at the Las Vegas Grand Prix. Several drivers won't be returning in 2025; The 2024 Formula 1 World Championship, with its record calendar of 24 races, concluded on Sunday with the Abu Dhabi Grand Prix, held under the lights at ... Below are the full results from the 2024 Formula 1 Las Vegas Grand Prix: 1) George Russell, Mercedes-Benz AMG 2) Lewis Hamilton, Mercedes-Benz AMG +7.313 seconds 3) Carlos Sainz, Ferrari +11.906 ... TEMPO.CO, Jakarta - Racing for Red Bull, Max Verstappen, successfully defended the Formula 1 champion title after finishing fifth in the Las Vegas Grand Prix at Las Vegas Strip Circuit, Nevada, on Sunday, November 24, 2024. He secured the title for the fourth consecutive time. Finishing fifth was enough for Verstappen to secure the championship as his McLaren F1 Team rival, Lando Norris, could ... Formula 1's season finale saw McLaren finally end their wait for a Constructors Championship with Lando Norris' fourth win of 2024. ... Five Storylines To Look Out for at the 2024 Abu Dhabi GP ...\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# 3) Instantiate the real search tool\n",
        "ddg = DuckDuckGoSearchRun()   # returns top‚Äëk snippets\n",
        "\n",
        "# 4) Wrap it so LangGraph can invoke it\n",
        "@tool\n",
        "def search_web(query: str) -> str:\n",
        "    \"\"\"Run a DuckDuckGo search and return the top results.\"\"\"\n",
        "    return ddg.run(query)\n",
        "\n",
        "# 5) Build your ReAct agent (LangGraph)\n",
        "agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[search_web],\n",
        "    prompt=(\n",
        "        \"You are a serious research assistant. \"\n",
        "        \"Whenever you need external facts, call the search_web tool. \"\n",
        "        \"Do not hallucinate or answer without using it.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 6) Invoke it correctly (pass messages list)\n",
        "result = agent.invoke({\n",
        "    \"messages\": [\n",
        "        SystemMessage(content=\"Use search_web for any factual lookup.\"),\n",
        "        HumanMessage(content=\"Who won the 2024 Formula¬†1 championship?\")\n",
        "    ]\n",
        "})\n",
        "\n",
        "# 7) Extract the final ToolMessage for your answer\n",
        "for msg in reversed(result[\"messages\"]):\n",
        "    if isinstance(msg, ToolMessage):\n",
        "        print(\"F1 champ answer:\", msg.content)\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part¬†II ¬∑ CrewAI Agent Patterns Demo\n",
        "\n",
        "This section reproduces **all seven artefacts** of Anthropic‚Äôs ‚ÄúBuilding¬†Effective¬†Agents‚Äù within **CrewAI¬†v0.31+** using Groq Cloud LLMs.\n",
        "\n",
        "| Layer             | Artefact                                   | CrewAI construct                                     |\n",
        "|-------------------|--------------------------------------------|------------------------------------------------------|\n",
        "| Building‚Äëblock    | Augmented¬†LLM¬†(Tool¬†Use)                   | Single‚Äêagent crew with LangChain‚Äêwrapped tools       |\n",
        "| Workflows (5√ó)    | Prompt‚ÄëChaining, Parallel, Routing, Orchestrator‚ÄëWorker, Evaluator‚ÄëOptimizer | `Process.sequential`, async tasks, templates, `Process.hierarchical`, review loop |\n",
        "| Agent archetype   | Autonomous ReAct Loop                      | `Process.autonomous`                                 |\n",
        "\n",
        "> **Prerequisites**  \n",
        "> * Groq API key stored in Colab Secrets under `GROQ_API_KEY`  \n",
        "> * GPU runtime is **not** required"
      ],
      "metadata": {
        "id": "ksveCV0bi9xA"
      },
      "id": "ksveCV0bi9xA"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Point LangChain‚Äôs Groq client and CrewAI‚Äôs OpenAI‚Äêcompatible layer\n",
        "os.environ[\"OPENAI_API_KEY\"]  = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\""
      ],
      "metadata": {
        "id": "Us07_Psb4zwD"
      },
      "id": "Us07_Psb4zwD",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "resp = llm.invoke([HumanMessage(content=\"Hello, world!\")])\n",
        "print(\"LLM says:\", resp.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDuVpNo4ef-Y",
        "outputId": "fe68cc13-6e28-48a2-fe78-ae9aed2e47e6"
      },
      "id": "zDuVpNo4ef-Y",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM says: Hello, world! It's great to meet you! Is there something I can help you with, or would you like to chat?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 ¬∑ Augmented¬†LLM (Tool Use)\n",
        "\n",
        "Define simple calculator tools via LangChain‚Äôs `@tool` decorator, then wrap them in CrewAI `Tool` objects."
      ],
      "metadata": {
        "id": "nNViPlzfjQRV"
      },
      "id": "nNViPlzfjQRV"
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai.tools import tool\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "@tool(\"Multiply\")\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool(\"Divide\")\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a by b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "assistant = Agent(\n",
        "    name=\"CalcAgent\",\n",
        "    role=\"Arithmetic Specialist\",\n",
        "    goal=\"Compute only via tools\",\n",
        "    backstory=\"You may not compute any math yourself; call a tool instead.\",\n",
        "    llm=\"groq/llama-3.3-70b-versatile\",   # ‚Üê note the groq/ prefix\n",
        "    tools=[multiply, divide],\n",
        "    system_prompt=\"Invoke Multiply or Divide for every operation.\",\n",
        ")\n",
        "\n",
        "\n",
        "expr_task = Task(\n",
        "    name=\"ComputeExpression\",\n",
        "    description=\"Compute '{expr}' strictly via Multiply and Divide tools.\",\n",
        "    expected_output=\"String with the numeric result.\",\n",
        "    agent=assistant\n",
        ")\n",
        "\n",
        "calc_crew = Crew(\n",
        "    agents=[assistant],\n",
        "    tasks=[expr_task],\n",
        "    process=Process.sequential,\n",
        "    max_rounds=4\n",
        ")\n",
        "\n",
        "# Kick off by passing a single dict of inputs:\n",
        "result = calc_crew.kickoff({\"expr\": \"9 * 8 / 6\"})\n",
        "print(\"Answer:\", result)  # should be \"12.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMisMXFq4yHZ",
        "outputId": "a94cdd8b-f444-444e-e2e4-f7e29e5b4f29"
      },
      "id": "pMisMXFq4yHZ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 ¬∑ Prompt-Chaining (Sequential)\n",
        "\n",
        "Chain two specialized Agents‚Äîone gathers bullet facts, the other writes a paragraph from them."
      ],
      "metadata": {
        "id": "26SLFAbDjW51"
      },
      "id": "26SLFAbDjW51"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è CrewAI ¬∑ Prompt-Chaining with ‚ÄúDataScout‚Äù Agent (Groq prefix)\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai.tools import tool\n",
        "\n",
        "# Reuse your tools if any, else just empty list\n",
        "# llm_model must include the groq/ prefix\n",
        "llm_model = \"groq/llama-3.3-70b-versatile\"\n",
        "\n",
        "# 1) Define Agents with the prefixed llm\n",
        "data_scout = Agent(\n",
        "    name=\"DataScoutAgent\",\n",
        "    role=\"Data Scout\",\n",
        "    goal=\"Extract three key insights about a topic\",\n",
        "    backstory=(\n",
        "        \"You are a data scout who combs through information to pull out the most important facts.\"\n",
        "    ),\n",
        "    llm=llm_model,\n",
        "    tools=[],  # no tools for this pattern\n",
        "    system_prompt=\"Provide three bullet-point insights about the given topic.\"\n",
        ")\n",
        "\n",
        "writer = Agent(\n",
        "    name=\"WriterAgent\",\n",
        "    role=\"Content Creator\",\n",
        "    goal=\"Compose a paragraph using provided insights\",\n",
        "    backstory=(\n",
        "        \"You are a writer who transforms bullet points into engaging prose.\"\n",
        "    ),\n",
        "    llm=llm_model,\n",
        "    tools=[],\n",
        "    system_prompt=(\n",
        "        \"Write a concise paragraph that incorporates the previous bullet-point insights.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 2) Task definitions\n",
        "insights_task = Task(\n",
        "    name=\"GatherInsights\",\n",
        "    description=\"Gather three bullet-point insights about '{topic}'.\",\n",
        "    expected_output=\"A newline-separated list of three bullet points.\",\n",
        "    agent=data_scout\n",
        ")\n",
        "\n",
        "paragraph_task = Task(\n",
        "    name=\"WriteParagraph\",\n",
        "    description=\"Write a concise paragraph using the previously provided bullet insights.\",\n",
        "    expected_output=\"A coherent paragraph.\",\n",
        "    agent=writer\n",
        ")\n",
        "\n",
        "# 3) Build and execute the Crew\n",
        "prompt_chain_crew = Crew(\n",
        "    agents=[data_scout, writer],\n",
        "    tasks=[insights_task, paragraph_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "# 4) Kickoff with a dict of inputs (must be dict)\n",
        "output = prompt_chain_crew.kickoff({\"topic\": \"edge computing\"})\n",
        "print(\"Prompt-Chaining Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0dSl5gCf6We",
        "outputId": "eafb8e39-0750-4d12-d7fe-f5aba00ae54a"
      },
      "id": "X0dSl5gCf6We",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt-Chaining Output:\n",
            " Edge computing has emerged as a crucial technology in reducing latency by processing data closer to the source, which is particularly beneficial for applications that require real-time processing, such as autonomous vehicles, smart homes, and industrial automation. This approach not only enables faster data processing but also decreases the amount of data that needs to be transmitted to the cloud or a central data center, thereby reducing bandwidth costs and improving network efficiency. As a key component of IoT solutions, edge computing plays a vital role in enhancing the overall resilience and reliability of systems by distributing computing resources across a network of edge devices. This distributed architecture allows data processing to continue uninterrupted even if the connection to the central cloud or data center is lost, making edge computing an attractive option for mission-critical applications where downtime can have significant consequences. By leveraging the benefits of edge computing, organizations can create more efficient, reliable, and responsive systems that can support a wide range of innovative use cases and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 ¬∑ Parallelization (Async)\n",
        "\n",
        "Run multiple subtasks concurrently (haiku, story, joke) and then merge their results into one cohesive output."
      ],
      "metadata": {
        "id": "B7KAuFdijfJ4"
      },
      "id": "B7KAuFdijfJ4"
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "# Reuse your groq-prefixed model\n",
        "llm_model = \"groq/llama-3.3-70b-versatile\"\n",
        "\n",
        "# 1) Define three specialist agents for new subtasks\n",
        "summarizer = Agent(\n",
        "    name=\"SummarizerAgent\",\n",
        "    role=\"Concise Summarizer\",\n",
        "    goal=\"Condense information into two sentences\",\n",
        "    backstory=\"You distill complex topics into clear, two-sentence summaries.\",\n",
        "    llm=llm_model,\n",
        "    tools=[],\n",
        "    system_prompt=\"Provide a two-sentence summary of the given topic.\"\n",
        ")\n",
        "\n",
        "analogist = Agent(\n",
        "    name=\"AnalogistAgent\",\n",
        "    role=\"Creative Analogist\",\n",
        "    goal=\"Generate an illustrative analogy for the topic\",\n",
        "    backstory=\"You craft vivid analogies that make abstract ideas tangible.\",\n",
        "    llm=llm_model,\n",
        "    tools=[],\n",
        "    system_prompt=\"Give an analogy that explains the topic in relatable terms.\"\n",
        ")\n",
        "\n",
        "factoid = Agent(\n",
        "    name=\"FactoidAgent\",\n",
        "    role=\"Fun-Fact Expert\",\n",
        "    goal=\"Share a surprising or interesting fact about the topic\",\n",
        "    backstory=\"You love uncovering and sharing fun facts that delight readers.\",\n",
        "    llm=llm_model,\n",
        "    tools=[],\n",
        "    system_prompt=\"Offer one surprising or little-known fact about the topic.\"\n",
        ")\n",
        "\n",
        "# 2) Launch the three subtasks in parallel\n",
        "summary_task = Task(\n",
        "    name=\"Summary\",\n",
        "    description=\"Write a two-sentence summary of '{topic}'.\",\n",
        "    expected_output=\"Two coherent sentences.\",\n",
        "    agent=summarizer,\n",
        "    async_execution=True\n",
        ")\n",
        "analogy_task = Task(\n",
        "    name=\"Analogy\",\n",
        "    description=\"Create an analogy that illustrates '{topic}'.\",\n",
        "    expected_output=\"One clear analogy.\",\n",
        "    agent=analogist,\n",
        "    async_execution=True\n",
        ")\n",
        "fact_task    = Task(\n",
        "    name=\"FunFact\",\n",
        "    description=\"Provide one interesting fact about '{topic}'.\",\n",
        "    expected_output=\"A single fun fact sentence.\",\n",
        "    agent=factoid,\n",
        "    async_execution=True\n",
        ")\n",
        "\n",
        "# 3) Merge step: aggregate all three outputs\n",
        "merge_task = Task(\n",
        "    name=\"MergeResults\",\n",
        "    description=(\n",
        "        \"Combine the summary, analogy, and fun fact into one cohesive block of text.\"\n",
        "    ),\n",
        "    expected_output=\"A single string containing all three parts.\",\n",
        "    agent=summarizer  # reuse SummarizerAgent or any agent\n",
        ")\n",
        "\n",
        "# 4) Build & run the Crew\n",
        "parallel_crew = Crew(\n",
        "    agents=[summarizer, analogist, factoid],\n",
        "    tasks=[summary_task, analogy_task, fact_task, merge_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "result = parallel_crew.kickoff({\"topic\": \"quantum computing\"})\n",
        "print(\"Parallelization Output:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExMBgKFajrIa",
        "outputId": "d2a8ffd2-83b2-400c-ab5b-a7ae735fe291"
      },
      "id": "ExMBgKFajrIa",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallelization Output:\n",
            " Quantum computing is a revolutionary technology that uses the principles of quantum mechanics to perform calculations and operations on data, enabling the potential to solve complex problems that are currently unsolvable with traditional computers. By harnessing the power of quantum bits or qubits, quantum computers can process vast amounts of information simultaneously, making them incredibly fast and potentially leading to breakthroughs in fields such as medicine, finance, and climate modeling. Imagine a vast library with an infinite number of books, each representing a possible solution to a complex problem, where a classical computer would be like a single reader who has to open each book one by one, while a quantum computer is like a magical reader who can open all the books simultaneously and find the correct solution instantly. In contrast to classical computing, where a reader would have to try each key one by one to open a lock, the quantum computer can try all the keys at the same time, thanks to the principles of superposition and entanglement, and when it \"reads\" the books, it can also see the connections between the pages, the relationships between the books, and even the patterns that emerge from the entire library. Additionally, the analogy of the library and the magical reader illustrates the power and potential of quantum computing, which can solve complex problems exponentially faster than classical computers and unlock new possibilities for fields such as cryptography, optimization, and artificial intelligence, and it's fascinating to note that a single quantum computer can process more calculations simultaneously than there are atoms in the observable universe, highlighting the immense potential of quantum computing to solve complex problems that are currently unsolvable with traditional computers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 ¬∑ Routing (Dynamic Dispatch)\n",
        "\n",
        "Classify the user query as ‚Äúsummary‚Äù, ‚Äúanalogy‚Äù, or ‚Äúfun_fact‚Äù and dispatch to the corresponding specialist Agent."
      ],
      "metadata": {
        "id": "AgjER90fjhll"
      },
      "id": "AgjER90fjhll"
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "# 1) Groq-prefixed model\n",
        "llm_model = \"groq/llama-3.3-70b-versatile\"\n",
        "\n",
        "# 2) Agents\n",
        "router = Agent(\n",
        "    name=\"RouterAgent\", role=\"Query Router\",\n",
        "    goal=\"Classify the query\", backstory=\"Decide summary/analogy/fun_fact.\",\n",
        "    llm=llm_model, tools=[], system_prompt=\"Label the user input as summary, analogy, or fun_fact.\"\n",
        ")\n",
        "summarizer = Agent(\n",
        "    name=\"SummarizerAgent\", role=\"Summarizer\",\n",
        "    goal=\"Summarize in two sentences\", backstory=\"...\",\n",
        "    llm=llm_model, tools=[], system_prompt=\"Two-sentence summary of the query.\"\n",
        ")\n",
        "analogist = Agent(\n",
        "    name=\"AnalogistAgent\", role=\"Analogist\",\n",
        "    goal=\"Craft an analogy\", backstory=\"...\",\n",
        "    llm=llm_model, tools=[], system_prompt=\"Analogy that explains the query.\"\n",
        ")\n",
        "factoid = Agent(\n",
        "    name=\"FactoidAgent\", role=\"Fact Provider\",\n",
        "    goal=\"Share a fun fact\", backstory=\"...\",\n",
        "    llm=llm_model, tools=[], system_prompt=\"One interesting fact about the query.\"\n",
        ")\n",
        "\n",
        "# 3) Tasks\n",
        "classify_task = Task(\n",
        "    name=\"ClassifyQuery\",\n",
        "    description=\"Label the user query as summary, analogy, or fun_fact.\",\n",
        "    expected_output=\"One of: summary, analogy, fun_fact\",\n",
        "    agent=router\n",
        ")\n",
        "\n",
        "summary_task = Task(\n",
        "    name=\"Summary\",\n",
        "    description=\"Generate a two-sentence summary of '{query}'.\",   # ‚Üê include {query}\n",
        "    expected_output=\"Two coherent sentences.\",\n",
        "    agent=summarizer\n",
        ")\n",
        "\n",
        "analogy_task = Task(\n",
        "    name=\"Analogy\",\n",
        "    description=\"Generate an analogy for '{query}'.\",              # ‚Üê include {query}\n",
        "    expected_output=\"One analogy.\",\n",
        "    agent=analogist\n",
        ")\n",
        "\n",
        "fact_task = Task(\n",
        "    name=\"FunFact\",\n",
        "    description=\"Generate one fun fact about '{query}'.\",         # ‚Üê include {query}\n",
        "    expected_output=\"One fact sentence.\",\n",
        "    agent=factoid\n",
        ")\n",
        "\n",
        "# 4) Crew\n",
        "routing_crew = Crew(\n",
        "    agents=[router, summarizer, analogist, factoid],\n",
        "    tasks=[classify_task, summary_task, analogy_task, fact_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "# 5) Kickoff\n",
        "routing_crew.kickoff({\"query\": \"Explain time dilation in relativity\"})\n",
        "\n",
        "# 6) Extract outputs from each Task object\n",
        "label = classify_task.output.raw.strip().lower()\n",
        "mapping = {\n",
        "    \"summary\": summary_task.output.raw,\n",
        "    \"analogy\": analogy_task.output.raw,\n",
        "    \"fun_fact\": fact_task.output.raw,\n",
        "}\n",
        "final = mapping.get(label, f\"‚ùå Unknown classification: {label}\")\n",
        "\n",
        "print(\"Routing Output:\\n\", final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yojt6G15klGr",
        "outputId": "70e9c11f-8a4c-4b9e-cbe1-8ee421edfee8"
      },
      "id": "Yojt6G15klGr",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Routing Output:\n",
            " The theory of relativity explains that time dilation occurs when an object's speed and proximity to a gravitational field cause time to pass slower for an observer in motion or near a strong gravitational field, resulting in a relative difference in the passage of time compared to a stationary observer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 ¬∑ Orchestrator-Worker (Simulated Hierarchical)\n",
        "\n",
        "Since the built-in `Process.hierarchical` in CrewAI is still maturing, here‚Äôs a robust Orchestrator-Worker pattern implemented with two simple crews and a small orchestration layer in Python:\n",
        "\n",
        "1. **Orchestrator (Manager) Crew** ‚Üí plans an outline of sections  \n",
        "2. **Worker Crew** ‚Üí writes each section in parallel via `kickoff_for_each`  \n",
        "3. **Merge** ‚Üí stitch all section texts into a final report  \n",
        "\n",
        "This mirrors a manager delegating work to a bench of writers and then consolidating their outputs."
      ],
      "metadata": {
        "id": "1DpC92OsjkeX"
      },
      "id": "1DpC92OsjkeX"
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "import json\n",
        "\n",
        "llm = \"groq/llama-3.3-70b-versatile\"\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ 2) Define the Manager Agent & Crew to plan sections ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "planner = Agent(\n",
        "    name=\"PlannerAgent\",\n",
        "    role=\"Outline Specialist\",\n",
        "    goal=\"Produce a JSON list of sections (title+description) for a report on the topic\",\n",
        "    backstory=\"You create an ordered outline of a technical report.\",\n",
        "    llm=llm,\n",
        "    tools=[],\n",
        "    system_prompt=\"Return a JSON array of objects with keys 'title' and 'description'.\"\n",
        ")\n",
        "\n",
        "plan_task = Task(\n",
        "    name=\"PlanSections\",\n",
        "    description=\"Plan 3‚Äì4 sections for a report on '{topic}', in JSON format.\",\n",
        "    expected_output=\"A JSON array like [{\\\"title\\\":‚Ä¶, \\\"description\\\":‚Ä¶}, ‚Ä¶]\",\n",
        "    agent=planner\n",
        ")\n",
        "\n",
        "plan_crew = Crew(\n",
        "    agents=[planner],\n",
        "    tasks=[plan_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "# Execute the planner\n",
        "plan_output = plan_crew.kickoff({\"topic\": \"CrewAI hierarchical demo\"})\n",
        "sections_json = plan_task.output.raw\n",
        "print(\"Outline JSON:\", sections_json)\n",
        "\n",
        "# Parse the outline into Python\n",
        "sections = json.loads(sections_json)\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ 3) Define the Writer Agent & Worker Crew ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "writer = Agent(\n",
        "    name=\"WriterAgent\",\n",
        "    role=\"Section Writer\",\n",
        "    goal=\"Write a markdown-formatted section from title + description\",\n",
        "    backstory=\"You write clear, concise technical sections from a title and a prompt.\",\n",
        "    llm=llm,\n",
        "    tools=[],\n",
        "    system_prompt=\"Use the title and description to write one markdown section.\"\n",
        ")\n",
        "\n",
        "write_task = Task(\n",
        "    name=\"WriteSection\",\n",
        "    description=\"Write section '# {title}' with: {description}\",\n",
        "    expected_output=\"One markdown-formatted section (title + body).\",\n",
        "    agent=writer\n",
        ")\n",
        "\n",
        "write_crew = Crew(\n",
        "    agents=[writer],\n",
        "    tasks=[write_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "# Kick off one sub-crew per section\n",
        "inputs = [\n",
        "    {\"title\": sec[\"title\"], \"description\": sec[\"description\"]}\n",
        "    for sec in sections\n",
        "]\n",
        "sections_bodies = write_crew.kickoff_for_each(inputs=inputs)\n",
        "\n",
        "# sections_bodies is a list of CrewOutput objects\n",
        "section_texts = []\n",
        "for sec in sections_bodies:\n",
        "    # each sec might be a CrewOutput or TaskOutput; `.raw` gives you the string\n",
        "    if hasattr(sec, \"raw\"):\n",
        "        section_texts.append(sec.raw)\n",
        "    elif hasattr(sec, \"output\") and hasattr(sec.output, \"raw\"):\n",
        "        section_texts.append(sec.output.raw)\n",
        "    else:\n",
        "        # fallback to str()\n",
        "        section_texts.append(str(sec))\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ 4) Merge all sections into a final report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "final_report = \"\\n\\n---\\n\\n\".join(section_texts)\n",
        "print(\"\\n\\nüìù Final Report:\\n\", final_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ljhqfAhpPqp",
        "outputId": "94f5e1db-9903-4cdd-a8b7-35598cb0dfa3"
      },
      "id": "_ljhqfAhpPqp",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outline JSON: [\n",
            "  {\n",
            "    \"title\": \"Introduction to CrewAI Hierarchical Demo\",\n",
            "    \"description\": \"This section will provide an overview of the CrewAI hierarchical demo, including its purpose, scope, and objectives. It will introduce the concept of hierarchical demonstrations and their importance in showcasing the capabilities of CrewAI.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"System Architecture and Design\",\n",
            "    \"description\": \"This section will delve into the system architecture and design of the CrewAI hierarchical demo, highlighting the key components, interfaces, and technologies used to build the demonstration. It will also discuss the design considerations and trade-offs made during the development process.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Features and Functionality\",\n",
            "    \"description\": \"This section will describe the features and functionality of the CrewAI hierarchical demo, including any notable use cases, scenarios, or test results. It will highlight the demo's capabilities, such as decision-making, problem-solving, or learning, and explain how they are demonstrated in the hierarchical structure.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Conclusion and Future Directions\",\n",
            "    \"description\": \"This section will summarize the key takeaways from the CrewAI hierarchical demo and discuss potential future directions for development and improvement. It will also reflect on the lessons learned from the demo and how they can be applied to real-world applications or further research.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "\n",
            "üìù Final Report:\n",
            " # Introduction to CrewAI Hierarchical Demo\n",
            "This section will provide an overview of the CrewAI hierarchical demo, including its purpose, scope, and objectives. It will introduce the concept of hierarchical demonstrations and their importance in showcasing the capabilities of CrewAI. The hierarchical demo is designed to illustrate the potential of CrewAI in a structured and organized manner, highlighting its key features and applications. By exploring the hierarchical demo, users can gain a deeper understanding of how CrewAI can be utilized to achieve specific goals and objectives, and how it can be integrated into various systems and processes. The objectives of the CrewAI hierarchical demo include demonstrating the versatility and adaptability of CrewAI, as well as its ability to handle complex tasks and scenarios. Overall, the CrewAI hierarchical demo is an essential tool for showcasing the capabilities and potential of CrewAI, and for providing users with a comprehensive understanding of its features and applications.\n",
            "\n",
            "---\n",
            "\n",
            "# System Architecture and Design\n",
            "This section will delve into the system architecture and design of the CrewAI hierarchical demo, highlighting the key components, interfaces, and technologies used to build the demonstration. It will also discuss the design considerations and trade-offs made during the development process. The CrewAI hierarchical demo is designed as a modular system, consisting of multiple layers and components that work together to provide a seamless user experience. The key components of the system include the user interface, business logic layer, data storage, and external interfaces. The system is built using a combination of technologies, including front-end frameworks, back-end programming languages, and database management systems. The design considerations and trade-offs made during the development process include scalability, performance, security, and usability. Overall, the system architecture and design of the CrewAI hierarchical demo are designed to provide a robust, flexible, and maintainable platform for demonstrating the capabilities of CrewAI.\n",
            "\n",
            "---\n",
            "\n",
            "# Features and Functionality\n",
            "This section describes the features and functionality of the CrewAI hierarchical demo, including any notable use cases, scenarios, or test results. The demo's capabilities, such as decision-making, problem-solving, or learning, are highlighted and explained in the context of the hierarchical structure. The CrewAI hierarchical demo is designed to demonstrate a range of features, including:\n",
            "* **Autonomous Decision-Making**: The demo showcases the ability of the CrewAI system to make decisions based on input from various sources, such as sensors, user input, and external data.\n",
            "* **Problem-Solving**: The hierarchical structure of the demo allows for the demonstration of complex problem-solving scenarios, where the system must navigate through multiple layers of decision-making to reach a solution.\n",
            "* **Machine Learning**: The demo integrates machine learning algorithms to enable the CrewAI system to learn from experience and adapt to new situations, improving its performance over time.\n",
            "* **Scalability**: The hierarchical structure of the demo allows for the easy addition of new features and functionality, making it a scalable solution for a range of applications.\n",
            "Notable use cases and scenarios demonstrated in the CrewAI hierarchical demo include:\n",
            "* **Emergency Response**: The demo shows how the CrewAI system can quickly respond to emergency situations, such as a system failure or natural disaster, by activating the appropriate response protocols.\n",
            "* **Resource Allocation**: The demo demonstrates how the CrewAI system can optimize resource allocation, such as assigning tasks to team members or allocating equipment, to achieve maximum efficiency.\n",
            "* **Mission Planning**: The demo showcases the ability of the CrewAI system to plan and execute complex missions, taking into account a range of factors, such as weather, terrain, and resource availability.\n",
            "The test results of the CrewAI hierarchical demo have shown promising outcomes, including:\n",
            "* **Improved Response Times**: The demo has demonstrated significant improvements in response times to emergency situations, compared to traditional manual response methods.\n",
            "* **Increased Efficiency**: The demo has shown that the CrewAI system can optimize resource allocation, leading to increased efficiency and productivity.\n",
            "* **Enhanced Situational Awareness**: The demo has demonstrated the ability of the CrewAI system to provide enhanced situational awareness, enabling better decision-making and problem-solving. Overall, the CrewAI hierarchical demo provides a comprehensive showcase of the features and functionality of the CrewAI system, highlighting its capabilities and potential applications in a range of fields.\n",
            "\n",
            "---\n",
            "\n",
            "# Conclusion and Future Directions\n",
            "This section will summarize the key takeaways from the CrewAI hierarchical demo and discuss potential future directions for development and improvement. It will also reflect on the lessons learned from the demo and how they can be applied to real-world applications or further research. The CrewAI hierarchical demo has demonstrated the potential of AI-powered systems in improving crew management and decision-making. The key takeaways from the demo include the importance of hierarchical planning, the need for real-time data processing, and the potential for AI to augment human decision-making. Future directions for development and improvement may include integrating the system with other AI-powered tools, expanding the system to accommodate larger crews, and exploring applications in other industries. The lessons learned from the demo can be applied to real-world applications, such as crew management in the aviation or maritime industries, and can also inform further research into the development of AI-powered decision support systems. Overall, the CrewAI hierarchical demo has shown that AI-powered systems have the potential to revolutionize crew management and decision-making, and further development and research are needed to fully realize this potential.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 ¬∑ Evaluator-Optimizer (Review Loop)\n",
        "\n",
        "Loop: the **AuthorAgent** drafts a tweet, the **CriticAgent** grades it good/bad with feedback, and the author rewrites if needed‚Äîup to 3 passes."
      ],
      "metadata": {
        "id": "cDgrfzpOjnrO"
      },
      "id": "cDgrfzpOjnrO"
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "# 1) Reuse your Groq-prefixed model\n",
        "llm_model = \"groq/llama-3.3-70b-versatile\"\n",
        "\n",
        "# 2) Define the Author and Critic agents\n",
        "author = Agent(\n",
        "    name=\"AuthorAgent\",\n",
        "    role=\"Content Author\",\n",
        "    goal=\"Write concise, engaging tweets about a topic\",\n",
        "    backstory=\"You draft social media posts that are catchy and informative.\",\n",
        "    llm=llm_model,\n",
        "    tools=[],\n",
        "    system_prompt=\"Write a tweet about the given topic.\"\n",
        ")\n",
        "\n",
        "critic = Agent(\n",
        "    name=\"CriticAgent\",\n",
        "    role=\"Feedback Critic\",\n",
        "    goal=\"Evaluate tweets and provide constructive feedback\",\n",
        "    backstory=\"You judge tweets as ‚Äògood‚Äô or ‚Äòbad‚Äô and explain why.\",\n",
        "    llm=llm_model,\n",
        "    tools=[],\n",
        "    system_prompt=\"Grade the last tweet as 'good' or 'bad' and explain your reasoning.\"\n",
        ")\n",
        "\n",
        "# 3) Set up the three tasks\n",
        "draft_task = Task(\n",
        "    name=\"DraftTweet\",\n",
        "    description=\"Draft a tweet about '{topic}'.\",\n",
        "    expected_output=\"A single tweet string.\",\n",
        "    agent=author\n",
        ")\n",
        "\n",
        "review_task = Task(\n",
        "    name=\"ReviewTweet\",\n",
        "    description=\"Review the last drafted tweet and provide a grade and feedback.\",\n",
        "    expected_output=\"A text starting with 'good:' or 'bad:' followed by feedback.\",\n",
        "    agent=critic\n",
        ")\n",
        "\n",
        "rewrite_task = Task(\n",
        "    name=\"RewriteTweet\",\n",
        "    description=(\n",
        "        \"Revise the previously drafted tweet using the critic's feedback \"\n",
        "        \"if it was graded 'bad'; otherwise return the original tweet.\"\n",
        "    ),\n",
        "    expected_output=\"The final tweet string.\",\n",
        "    agent=author\n",
        ")\n",
        "\n",
        "# 4) Build and run the crew with up to 3 passes\n",
        "eval_crew = Crew(\n",
        "    agents=[author, critic],\n",
        "    tasks=[draft_task, review_task, rewrite_task],\n",
        "    process=Process.sequential,\n",
        "    max_rounds=3\n",
        ")\n",
        "\n",
        "# 5) Kickoff\n",
        "output = eval_crew.kickoff({\"topic\": \"LangGraph\"})\n",
        "print(\"Evaluator-Optimizer Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBau_fRXsYZa",
        "outputId": "1270d1a5-a80b-48fe-fc51-fb051b07b7a4"
      },
      "id": "SBau_fRXsYZa",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluator-Optimizer Output:\n",
            " Discover the power of #LangGraph, a revolutionary AI model that's changing the game in language understanding and generation! With its advanced capabilities, LangGraph is enabling new possibilities in natural language processing and beyond, such as improving chatbots, enhancing language translation, and generating creative content! #AI #NLP #Innovation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 ¬∑ Autonomous Agent (Tool‚ÄêCalling Loop)\n",
        "\n",
        "Simulate an autonomous ReAct‚Äêstyle agent by using a single‚Äêtask crew with `Process.sequential` and `max_rounds`‚Äîthe agent will call tools iteratively until no more calls are needed."
      ],
      "metadata": {
        "id": "uA-7amTHjqy9"
      },
      "id": "uA-7amTHjqy9"
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai.tools import tool\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "\n",
        "# Use the same Groq‚Äêprefixed model string\n",
        "llm_model = \"groq/llama-3.3-70b-versatile\"\n",
        "\n",
        "# 1) Define calculator tools\n",
        "@tool(\"Multiply\")\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Return the product of two integers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool(\"Add\")\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Return the sum of two integers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool(\"Divide\")\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide the first integer by the second and return a float.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "# 2) Build the autonomous calculator Agent\n",
        "assistant = Agent(\n",
        "    name=\"AutoCalcAgent\",\n",
        "    role=\"Autonomous Calculator\",\n",
        "    goal=\"Compute arithmetic expressions by invoking tools for each operation\",\n",
        "    backstory=(\n",
        "        \"You are an autonomous calculation assistant. \"\n",
        "        \"For every expression, you must call Multiply, Add or Divide via tools‚Äînever compute directly.\"\n",
        "    ),\n",
        "    llm=llm_model,\n",
        "    tools=[multiply, add, divide],\n",
        "    system_prompt=\"Use the tools for each arithmetic step. Do not calculate internally.\"\n",
        ")\n",
        "\n",
        "# 3) Define the single looping Task\n",
        "expr_task = Task(\n",
        "    name=\"ComputeExpression\",\n",
        "    description=\"Compute the expression '{expr}' step by step using the tools.\",\n",
        "    expected_output=\"The final numeric result as a string.\",\n",
        "    agent=assistant\n",
        ")\n",
        "\n",
        "# 4) Create and run the Crew with up to 4 tool-LLM cycles\n",
        "auto_crew = Crew(\n",
        "    agents=[assistant],\n",
        "    tasks=[expr_task],\n",
        "    process=Process.sequential,\n",
        "    max_rounds=4\n",
        ")\n",
        "\n",
        "# 5) Kickoff the autonomous agent\n",
        "output = auto_crew.kickoff({\"expr\": \"9 * 8 / 6\"})\n",
        "print(\"Autonomous Agent Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkOtC4f4tABf",
        "outputId": "2cd447ae-727c-4d70-96d6-5c77f29982af"
      },
      "id": "vkOtC4f4tABf",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous Agent Output: 12.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}